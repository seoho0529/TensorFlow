{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqE59Ydl+4chXhG3DDVQ8X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kbZXus2KnR78"
      },
      "outputs": [],
      "source": [
        "# 디코더에서 출력단어를 예측하는데 매 시점마다 인코더에서 전체 입력문장을 다시 참조하는 방식\n",
        "# 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어에 좀 더 집중해서 작업한다.\n",
        "# seq2seq 알고리즘에 문제점 중 일부를 개선\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, LSTM, Dense, Concatenate, Attention\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가상의 파라미터에 대한 초기값\n",
        "input_length = 10\n",
        "output_length = 10\n",
        "vocab_size = 1000\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "# encoder 정의\n",
        "encoder_inputs = Input(shape=(input_length, embedding_dim))\n",
        "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "encoder_outputs,_,_ = encoder_lstm(encoder_inputs) # 넘어오는건 3갠데 한개만 사용\n",
        "\n",
        "# decoder 정의\n",
        "decoder_inputs = Input(shape=(output_length, embedding_dim))\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True)\n",
        "decoder_outputs = decoder_lstm(decoder_inputs)\n",
        "\n",
        "# Attention 레이어\n",
        "attention_layer = Attention()\n",
        "attention_output = attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Attention 레이어는 decoder의 출력과 encoder의 출력 사이에 관계를 계산하여 중요 정보에 집중할 수 있도록 도움을 준다.\n",
        "concat_layer = Concatenate(axis=-1)\n",
        "docoder_concat_input = concat_layer([decoder_outputs,attention_output])\n",
        "\n",
        "# 출력 레이어 : 최종적으로 Dense를 통해 예측을 수행한다.\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(docoder_concat_input)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBOibK8AoTl-",
        "outputId": "6daf5d0b-fa8c-4f64-e295-e980049ac5c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 10, 64)]             0         []                            \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 10, 64)]             0         []                            \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               (None, 10, 128)              98816     ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               [(None, 10, 128),            98816     ['input_3[0][0]']             \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 10, 128)              0         ['lstm_3[0][0]',              \n",
            "                                                                     'lstm_2[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 10, 256)              0         ['lstm_3[0][0]',              \n",
            "                                                                     'attention[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 10, 1000)             257000    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 454632 (1.73 MB)\n",
            "Trainable params: 454632 (1.73 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}