{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMJZt1ARBqNk2G7Db4cw74H"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s56BbkfUFgxQ"
      },
      "outputs": [],
      "source": [
        "# imdb dataset으로 감성 분류\n",
        "# Transformer는 입력 데이터들간의 상호작용을 고려하는 self-attention\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder block\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    res = x + inputs  # Ensure shapes are compatible\n",
        "\n",
        "    # Feed Forward NN\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res  # Ensure shapes are compatible\n",
        "\n",
        "def build_sentiment_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    embedding_layer = layers.Embedding(input_dim=10000, output_dim=64, input_length=input_shape[0])(inputs)\n",
        "    x = embedding_layer + tf.random.normal(shape=tf.shape(embedding_layer))\n",
        "\n",
        "    # Transformer blocks\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # MLP\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation='relu')(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "iddecKlRLHNh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train),(x_val, y_val) = keras.datasets.imdb.load_data(num_words=10000)\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=100)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=100)\n",
        "\n",
        "# example usage\n",
        "input_shape=(100,)\n",
        "head_size = 256\n",
        "num_heads = 4\n",
        "ft_dim = 4\n",
        "num_transformer_blocks = 4\n",
        "mlp_units = [128]\n",
        "dropout = 0.25\n",
        "mlp_dropout=0.25\n",
        "\n",
        "model = build_sentiment_model(\n",
        "    input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout, mlp_dropout\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daWUDZpuLIzc",
        "outputId": "fff5d0ec-5d30-4200-e27d-93ef613884d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)     (None, 100, 64)              640000    ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape_3 (TFOp  (3,)                         0         ['embedding_3[0][0]']         \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.random.normal_3 (TFOpLa  (None, 100, 64)              0         ['tf.compat.v1.shape_3[0][0]']\n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TF  (None, 100, 64)              0         ['embedding_3[0][0]',         \n",
            " OpLambda)                                                           'tf.random.normal_3[0][0]']  \n",
            "                                                                                                  \n",
            " layer_normalization (Layer  (None, 100, 64)              128       ['tf.__operators__.add_3[0][0]\n",
            " Normalization)                                                     ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention (Mult  (None, 100, 64)              265280    ['layer_normalization[0][0]', \n",
            " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TF  (None, 100, 64)              0         ['multi_head_attention[0][0]',\n",
            " OpLambda)                                                           'tf.__operators__.add_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_1 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_4[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 100, 4)               260       ['layer_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 100, 4)               0         ['conv1d[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 100, 64)              320       ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TF  (None, 100, 64)              0         ['conv1d_1[0][0]',            \n",
            " OpLambda)                                                           'tf.__operators__.add_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_2 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_5[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (Mu  (None, 100, 64)              265280    ['layer_normalization_2[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TF  (None, 100, 64)              0         ['multi_head_attention_1[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'tf.__operators__.add_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_3 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_6[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 100, 4)               260       ['layer_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 100, 4)               0         ['conv1d_2[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 100, 64)              320       ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TF  (None, 100, 64)              0         ['conv1d_3[0][0]',            \n",
            " OpLambda)                                                           'tf.__operators__.add_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_7[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (Mu  (None, 100, 64)              265280    ['layer_normalization_4[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TF  (None, 100, 64)              0         ['multi_head_attention_2[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'tf.__operators__.add_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_8[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 100, 4)               260       ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 100, 4)               0         ['conv1d_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 100, 64)              320       ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TF  (None, 100, 64)              0         ['conv1d_5[0][0]',            \n",
            " OpLambda)                                                           'tf.__operators__.add_8[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_6 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_9[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (Mu  (None, 100, 64)              265280    ['layer_normalization_6[0][0]'\n",
            " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (T  (None, 100, 64)              0         ['multi_head_attention_3[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'tf.__operators__.add_9[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_10[0][0\n",
            " erNormalization)                                                   ]']                           \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, 100, 4)               260       ['layer_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 100, 4)               0         ['conv1d_6[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, 100, 64)              320       ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (T  (None, 100, 64)              0         ['conv1d_7[0][0]',            \n",
            " FOpLambda)                                                          'tf.__operators__.add_10[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d (  (None, 64)                   0         ['tf.__operators__.add_11[0][0\n",
            " GlobalAveragePooling1D)                                            ]']                           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  8320      ['global_average_pooling1d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 128)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1)                    129       ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1712913 (6.53 MB)\n",
            "Trainable params: 1712913 (6.53 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-4), metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=32, verbose=2)\n",
        "print(history.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZy9HcXLuts_",
        "outputId": "66f070a9-e67d-428e-b302-1b0155471770"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 - 86s - loss: 0.6959 - acc: 0.5033 - val_loss: 0.6919 - val_acc: 0.5229 - 86s/epoch - 110ms/step\n",
            "Epoch 2/10\n",
            "782/782 - 42s - loss: 0.6833 - acc: 0.5524 - val_loss: 0.6641 - val_acc: 0.5930 - 42s/epoch - 54ms/step\n",
            "Epoch 3/10\n",
            "782/782 - 40s - loss: 0.6418 - acc: 0.6302 - val_loss: 0.6302 - val_acc: 0.6510 - 40s/epoch - 51ms/step\n",
            "Epoch 4/10\n",
            "782/782 - 39s - loss: 0.6041 - acc: 0.6715 - val_loss: 0.6000 - val_acc: 0.6752 - 39s/epoch - 49ms/step\n",
            "Epoch 5/10\n",
            "782/782 - 37s - loss: 0.5733 - acc: 0.7004 - val_loss: 0.5780 - val_acc: 0.6989 - 37s/epoch - 47ms/step\n",
            "Epoch 6/10\n",
            "782/782 - 35s - loss: 0.5530 - acc: 0.7170 - val_loss: 0.5577 - val_acc: 0.7166 - 35s/epoch - 45ms/step\n",
            "Epoch 7/10\n",
            "782/782 - 39s - loss: 0.5292 - acc: 0.7360 - val_loss: 0.5378 - val_acc: 0.7278 - 39s/epoch - 50ms/step\n",
            "Epoch 8/10\n",
            "782/782 - 38s - loss: 0.5121 - acc: 0.7455 - val_loss: 0.5191 - val_acc: 0.7414 - 38s/epoch - 49ms/step\n",
            "Epoch 9/10\n",
            "782/782 - 36s - loss: 0.4933 - acc: 0.7602 - val_loss: 0.5086 - val_acc: 0.7514 - 36s/epoch - 46ms/step\n",
            "Epoch 10/10\n",
            "782/782 - 37s - loss: 0.4809 - acc: 0.7710 - val_loss: 0.4953 - val_acc: 0.7569 - 37s/epoch - 47ms/step\n",
            "{'loss': [0.6958608031272888, 0.6832625865936279, 0.6417998671531677, 0.6040773987770081, 0.5733204483985901, 0.5530045628547668, 0.5291905999183655, 0.5121396780014038, 0.4933367967605591, 0.48085227608680725], 'acc': [0.503279983997345, 0.5523999929428101, 0.6302000284194946, 0.6715199947357178, 0.7003999948501587, 0.7170400023460388, 0.7359600067138672, 0.7454800009727478, 0.7601600289344788, 0.7709599733352661], 'val_loss': [0.6918882727622986, 0.6641219258308411, 0.6302341222763062, 0.6000241637229919, 0.5779856443405151, 0.5577069520950317, 0.537826657295227, 0.5190738439559937, 0.5086036324501038, 0.49525776505470276], 'val_acc': [0.5229200124740601, 0.5930399894714355, 0.6510000228881836, 0.6752399802207947, 0.6988800168037415, 0.7166399955749512, 0.727840006351471, 0.7414000034332275, 0.7514399886131287, 0.756879985332489]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sre_constants import MAX_REPEAT\n",
        "text_sample = [\"I loved movie\", \"Tis movie sucks, It's so boring.\"]\n",
        "max_len=100\n",
        "tok = keras.datasets.imdb.get_word_index()\n",
        "text_sequence = [[tok[word] if word in tok else 0 for word in sample.split()] for sample in text_sample]\n",
        "text_sequence = keras.preprocessing.sequence.pad_sequences(text_sequence, maxlen=max_len)\n",
        "# print(text_sequence)\n",
        "\n",
        "pred = model.predict(text_sequence)\n",
        "\n",
        "b_pred = (pred > 0.5).astype(int)\n",
        "\n",
        "for i, samp in enumerate(text_sample):\n",
        "  print(f\"sample : {samp}\")\n",
        "  print(f\"predicted : {'Positive' if b_pred[i] == 1 else 'Negative'}\")\n",
        "  print(f\"confidence : {pred[i][0]:.3f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brlnmefPv2uJ",
        "outputId": "d9b3df9c-a1ff-4b52-db0c-6932809fa0cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "sample : I loved movie\n",
            "predicted : Positive\n",
            "confidence : 0.725\n",
            "\n",
            "sample : Tis movie sucks, It's so boring.\n",
            "predicted : Positive\n",
            "confidence : 0.866\n",
            "\n"
          ]
        }
      ]
    }
  ]
}