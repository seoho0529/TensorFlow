{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFXhhFvwRWhieFShWsZppE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoho0529/TensorFlow/blob/main/tf46Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gyBC70KDvOWE"
      },
      "outputs": [],
      "source": [
        "# Transformer : RNN을 제거하고 Attention으로만 구성된 서로 다른 인코더와 디코더가 여러 개 쌓인 형태의 네트워크.\n",
        "# 인코더 또는 디코더 안에는 self attention과 feedford 신경망(keras : Dense)으로 구성 되어 있다.\n",
        "# self attention 함수는 주어진 Query에 대해서 key와 유사도를 구한다. 그리고 이 유사도를 키와 매핑되어 있는 값(value)에 반영한다.\n",
        "# self attention은 RNN 처럼 순서대로 처리 하는 방식이 아니라 해당하는 단어와 관련된 뜻을 찾기 위한 어텐션을 말한다.\n",
        "\n",
        "# 간단한 구조 이해\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "vocab_size = 20000\n",
        "maxlen=200\n",
        "\n",
        "(x_train,y_train),(x_val,y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), len(x_val))\n",
        "\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "print(x_train[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8UGFZzox3S_",
        "outputId": "7cbfcb46-83fd-4da2-981e-31bb49936326"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "25000 25000\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    1   14   47    8   30   31    7    4  249  108    7\n",
            "    4 5974   54   61  369   13   71  149   14   22  112    4 2401  311\n",
            "   12   16 3711   33   75   43 1829  296    4   86  320   35  534   19\n",
            "  263 4821 1301    4 1873   33   89   78   12   66   16    4  360    7\n",
            "    4   58  316  334   11    4 1716   43  645  662    8  257   85 1200\n",
            "   42 1228 2578   83   68 3912   15   36  165 1539  278   36   69    2\n",
            "  780    8  106   14 6905 1338   18    6   22   12  215   28  610   40\n",
            "    6   87  326   23 2300   21   23   22   12  272   40   57   31   11\n",
            "    4   22   47    6 2307   51    9  170   23  595  116  595 1352   13\n",
            "  191   79  638   89    2   14    9    8  106  607  624   35  534    6\n",
            "  227    7  129  113]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Token and Position embedding\n",
        "inputs = layers.Input(shape=(200,))\n",
        "x = layers.Embedding(input_dim=128, output_dim=32)(inputs)\n",
        "print(tf.keras.Model(inputs=inputs, outputs=x).summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNBl_rbay3HW",
        "outputId": "48154b86-1a30-49d7-d6b4-f4f825b00aa2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 32)           4096      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4096 (16.00 KB)\n",
            "Trainable params: 4096 (16.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(200,))\n",
        "x = layers.Embedding(input_dim=128, output_dim=32)(inputs)\n",
        "positions = tf.range(start=0, limit=200, delta=1)\n",
        "# 각 단어 위치에 따른 index값을 embedding하여 feature를 추출 한 후, 합을 구함(단어embedding 결과 + 위치embedding결과)\n",
        "positions = layers.Embedding(input_dim=200, output_dim=32)(positions)\n",
        "embedding_layer = x + positions\n",
        "\n",
        "# Multi-head attention을 위한 간단한 구조를 작성\n",
        "EmbeddingDim=5\n",
        "WordLen=5\n",
        "\n",
        "inputs = layers.Input(shape=(WordLen))\n",
        "positions = tf.range(start=0, limit=WordLen, delta=1)\n",
        "positions = layers.Embedding(input_dim=WordLen, output_dim=EmbeddingDim)(positions)\n",
        "x = layers.Embedding(input_dim=200, output_dim=EmbeddingDim)(inputs)\n",
        "embedding_layer = x + positions\n",
        "\n",
        "# num_heads=1 : 현재 Attention Matrix를 만드는 작업을 몇 번할 것인가를 결정\n",
        "attention_output = layers.MultiHeadAttention(num_heads=1, key_dim=1, use_bias=False)(embedding_layer, embedding_layer)\n",
        "print(tf.keras.Model(inputs=inputs, outputs=attention_output).summary())\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=attention_output)\n",
        "print([w.shape for w in model.get_weights()])\n",
        "# [(200, 5),               (5, 1, 1), (5, 1, 1),        (5, 1, 1),             (1, 1, 5)]\n",
        "# embedding_layer weight,     Query,      Key,      Value 에 대한 weight      dot production 내부에서 transpose되어 순서가 변경\n",
        "\n",
        "print('----------------------------------------------')\n",
        "# MultiHeadAttention layer에 num_heads=2, key_dim=3 으로 변경해보자\n",
        "inputs = layers.Input(shape=(WordLen,))\n",
        "positions = tf.range(start=0, limit=WordLen, delta=1)\n",
        "positions = layers.Embedding(input_dim=WordLen, output_dim=EmbeddingDim)(positions)\n",
        "x = layers.Embedding(input_dim=200, output_dim=EmbeddingDim)(inputs)\n",
        "embedding_layer = x + positions\n",
        "\n",
        "# num_heads=1 : 현재 Attention Matrix를 만드는 작업을 몇 번할 것인가를 결정\n",
        "# attention_output = layers.MultiHeadAttention(num_heads=2, key_dim=3, use_bias=False)(embedding_layer, embedding_layer)\n",
        "attention_output = layers.MultiHeadAttention(num_heads=2, key_dim=3, use_bias=True)(embedding_layer, embedding_layer)\n",
        "print(tf.keras.Model(inputs=inputs, outputs=attention_output).summary())\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=attention_output)\n",
        "print([w.shape for w in model.get_weights()])\n",
        "# [(200, 5), (5, 2, 3), (5, 2, 3), (5, 2, 3), (2, 3, 5)]\n",
        "# [(200, 5), (5, 2, 3), (2, 3), (5, 2, 3), (2, 3), (5, 2, 3), (2, 3), (2, 3, 5), (5,)]  use_bias=True하면 중간에 상수가 더해짐\n",
        "# 파라미터에 대한 존재 이유, 연산 등의 이해가 있다면, 다음으로 Scaled dot production Attention만 이해하면 Transformer에 기초 끝!!\n",
        "\n",
        "# 단어들의 유사도와 같은 역할을 하는 Attention score를 구하고 이를 Value에 곱해주는 작업을 Single-Head Attention이 한다.\n",
        "# Single-Head Attention을 여러개 병렬로 처리하면 이게 바로 Multi - Head Attention이 된다.\n",
        "# 이 때, num_heads는 이 Attention matrix를 만드는 수행을 몇 번 할 것인가를 의미함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MPMPWEkyktO",
        "outputId": "af893f56-4ec0-4e37-e4de-424d81e93cb7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_17 (InputLayer)       [(None, 5)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding_32 (Embedding)    (None, 5, 5)                 1000      ['input_17[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (T  (None, 5, 5)                 0         ['embedding_32[0][0]']        \n",
            " FOpLambda)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (Mu  (None, 5, 5)                 20        ['tf.__operators__.add_15[0][0\n",
            " ltiHeadAttention)                                                  ]',                           \n",
            "                                                                     'tf.__operators__.add_15[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1020 (3.98 KB)\n",
            "Trainable params: 1020 (3.98 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[(200, 5), (5, 1, 1), (5, 1, 1), (5, 1, 1), (1, 1, 5)]\n",
            "----------------------------------------------\n",
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_18 (InputLayer)       [(None, 5)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding_34 (Embedding)    (None, 5, 5)                 1000      ['input_18[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (T  (None, 5, 5)                 0         ['embedding_34[0][0]']        \n",
            " FOpLambda)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (Mu  (None, 5, 5)                 143       ['tf.__operators__.add_16[0][0\n",
            " ltiHeadAttention)                                                  ]',                           \n",
            "                                                                     'tf.__operators__.add_16[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1143 (4.46 KB)\n",
            "Trainable params: 1143 (4.46 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[(200, 5), (5, 2, 3), (2, 3), (5, 2, 3), (2, 3), (5, 2, 3), (2, 3), (2, 3, 5), (5,)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 MultiHeadAttention을 통과한 분류 모델을 작성\n",
        "EmbeddingDim = 128\n",
        "WordLen = 200\n",
        "\n",
        "inputs = layers.Input(shape=(WordLen,))\n",
        "positions = tf.range(start=0, limit=WordLen, delta=1)\n",
        "positions = layers.Embedding(input_dim=WordLen, output_dim=EmbeddingDim)(positions)\n",
        "x = layers.Embedding(input_dim=200, output_dim=EmbeddingDim)(inputs)\n",
        "embedding_layer = x + positions\n",
        "\n",
        "\n",
        "attention_output = layers.MultiHeadAttention(num_heads=1, key_dim=2, use_bias=True)(embedding_layer, embedding_layer)\n",
        "\n",
        "x = layers.GlobalAvgPool1D()(attention_output)\n",
        "# MultiHeadAttention을 통과한 output은 Embedding의 output과 동일하므로 GlobalAveragePooling1D을 통해 Vector화 할 수 있다.\n",
        "outputs = layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=attention_output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "print([w.shape for w in model.get_weights()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vigzcq6Z3l5A",
        "outputId": "00caf732-5511-467d-bd3b-04ea65f1bc73"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_20 (InputLayer)       [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_38 (Embedding)    (None, 200, 128)             25600     ['input_20[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (T  (None, 200, 128)             0         ['embedding_38[0][0]']        \n",
            " FOpLambda)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (M  (None, 200, 128)             1158      ['tf.__operators__.add_18[0][0\n",
            " ultiHeadAttention)                                                 ]',                           \n",
            "                                                                     'tf.__operators__.add_18[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 26758 (104.52 KB)\n",
            "Trainable params: 26758 (104.52 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "[(200, 128), (128, 1, 2), (1, 2), (128, 1, 2), (1, 2), (128, 1, 2), (1, 2), (1, 2, 128), (128,)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2utQZlih0iRj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}