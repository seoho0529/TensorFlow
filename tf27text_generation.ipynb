{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON6AswpzFgmRS7GQl1zigT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seoho0529/TensorFlow/blob/main/tf27text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPvXtkk4pYl1",
        "outputId": "795b6e1a-080e-47b4-a91b-7dae2a8775e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3, 6, 47, 48, 49, 2, 50, 51, 52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74, 75, 5, 76, 77, 78, 79, 25, 80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97, 10, 98, 99, 100, 101, 102, 25, 103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119, 120, 121, 11, 2, 13, 122, 3, 14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18, 137, 138, 139, 140, 141, 142, 3, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12, 158, 16, 159, 160, 161, 162, 163, 164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202, 36, 23, 203, 1, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]\n",
            "{'아파트': 1, '하락한': 2, '있다': 3, '최고가': 4, '9월': 5, '일주일': 6, '따르면': 7, '최근': 8, '대비': 9, '지난달': 10, '수억': 11, '떨어진': 12, '거래가': 13, '아실에': 14, '동안': 15, '가장': 16, '단지는': 17, '서울': 18, '전용': 19, '거래됐다': 20, '이는': 21, '2022년': 22, '거래된': 23, '보다': 24, '떨어졌다': 25, '아니다': 26, '지난': 27, '비해': 28, '실거래': 29, '이상': 30, '매매거래': 31, '회전율은': 32, '2': 33, '28': 34, '이후': 35, '매매': 36, '시장이': 37, \"'숨고르기'에\": 38, '들어간': 39, '가운데': 40, '부촌': 41, '1위인': 42, '서초는': 43, '물론': 44, '강남권에서도': 45, '잇따르고': 46, '새': 47, '매매가가': 48, '12억원': 49, '사례도': 50, '나왔다': 51, '4일': 52, '국토교통부': 53, '실거래가': 54, '시스템과': 55, '매매가격이': 56, '많이': 57, '강남구': 58, '도곡동의': 59, \"'도곡렉슬'로\": 60, '조사됐다': 61, '이': 62, '134': 63, '9㎡가': 64, '29일': 65, '37억2000만원에': 66, '5월': 67, '같은': 68, '면적': 69, '49억4000만원': 70, '12억2000만원': 71, '내려간': 72, '것': 73, '직전': 74, '거래인': 75, '40억원': 76, '과': 77, '비교해도': 78, '2억8000만원': 79, '‘부촌': 80, '1번지’': 81, '서초구도': 82, '예외는': 83, '반포동': 84, \"'래미안원베일리'\": 85, '전용84㎡가': 86, '38억5000만원에': 87, '거래가격인': 88, '43억원에': 89, '4억5000만원': 90, '것이다': 91, '29억3000만원에': 92, '주인이': 93, '바뀐': 94, '잠원동': 95, \"'래미안신반포팰리스'\": 96, '전용84㎡도': 97, '28억4000만원에': 98, '매매돼': 99, '두달': 100, '사이에': 101, '9000만원': 102, '송파구에서도': 103, '잠실동': 104, \"'잠실엘스'\": 105, '84': 106, '88㎡가': 107, '27일': 108, '22억9000만원에': 109, '거래돼': 110, '2021년': 111, '기록했던': 112, '27억원': 113, '4억1000만원': 114, '하락했다': 115, '이뿐만이': 116, '부산': 117, '등': 118, '지방': 119, '랜드마크': 120, '아파트에서도': 121, '포착되고': 122, '가격이': 123, '3억원': 124, '내린': 125, '전국에서': 126, '20여곳에': 127, '이른다': 128, '2억원': 129, '단지를': 130, '포함하면': 131, '60곳이': 132, '넘었다': 133, '한국부동산원': 134, '통계를': 135, '보면': 136, '강남': 137, '3구를': 138, '중심으로': 139, '대도시지역에서': 140, '하락폭이': 141, '커지고': 142, '박원갑': 143, 'kb국민은행': 144, '부동산': 145, '수석전문위원은': 146, '“올해': 147, '강남권': 148, '회복세가': 149, '다른': 150, '지역에': 151, '비해서': 152, '가파르게': 153, '진행되면서': 154, '현재는': 155, '가격': 156, '메리트가': 157, '점이': 158, '결정적이라고': 159, '볼': 160, '수': 161, '있다”고': 162, '분석했다': 163, '매매시장도': 164, '빠르게': 165, '냉각되고': 166, '직방에': 167, '올해': 168, '전국': 169, '3': 170, '04': 171, '로': 172, '에': 173, '0': 174, '76': 175, 'p': 176, '상승했다': 177, '신고가': 178, '최초': 179, '도입된': 180, '2006년': 181, '장기': 182, '시계열을': 183, '살펴보면': 184, '지난해': 185, '역대': 186, '두번째로': 187, '낮은': 188, '수치를': 189, '기록했다': 190, '거래': 191, '회전율이': 192, '5': 193, '이하를': 194, '기록한': 195, '경우는': 196, '2022년과': 197, '2023년': 198, '뿐이다': 199, '재고': 200, '가구수': 201, '실제': 202, '해당': 203, '거래량': 204, '비율을': 205, '뜻한다': 206, '함영진': 207, '직방': 208, '빅데이터랩장은': 209, '“올': 210, '상반기': 211, '반짝': 212, '회복된': 213, '일부지역의': 214, '회복흐름이': 215, '하반기': 216, '들어': 217, '다시': 218, '주춤한': 219, '모습이다”라며': 220, '고금리에': 221, '경기둔화와': 222, '주택가격': 223, '부담에': 224, '대한': 225, '우려로': 226, '위축세가': 227, '뚜렷하다”고': 228, '말했다': 229}\n"
          ]
        }
      ],
      "source": [
        "# RNN을 이용한 텍스트 생성\n",
        "# 문맥을 반영해서 다음 단어를 예측하여 텍스트를 생성 - 다항분류 (many to one에 해당)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences, to_categorical\n",
        "import numpy as np\n",
        "from keras.layers import Embedding, Dense, LSTM, Flatten\n",
        "from keras.models import Sequential\n",
        "'''\n",
        "text = \"\"\"거리에 하얀 눈이 쌓이고 있다\n",
        "그의 눈이 밝게 빛난다\n",
        "눈이 내리고 있는 오후 거리는 눈이 부시다\"\"\"  # label은 문장의 마지막에 있는 단어, ex) feature:거리, 하얀눈:label\n",
        "'''\n",
        "text = \"\"\"\n",
        "아파트 시장이 '숨고르기'에 들어간 가운데 부촌 1위인 서초는 물론 강남권에서도 수억 떨어진 거래가 잇따르고 있다. 일주일 새 매매가가 12억원 하락한 사례도 나왔다.\n",
        "4일 국토교통부 실거래가 시스템과 아실에 따르면 최근 일주일 동안 매매가격이 최고가 대비 가장 많이 하락한 단지는 서울 강남구 도곡동의 '도곡렉슬'로 조사됐다. 이 아파트 전용 134.9㎡가 지난달 29일 37억2000만원에 거래됐다. 이는 2022년 5월 거래된 같은 면적 최고가(49억4000만원)보다 12억2000만원 내려간 것. 직전 거래인 9월(40억원)과 비교해도 2억8000만원 떨어졌다.\n",
        "‘부촌 1번지’ 서초구도 예외는 아니다. 반포동 '래미안원베일리' 전용84㎡가 최근 38억5000만원에 거래됐다. 이는 지난 9월 거래가격인 43억원에 비해 4억5000만원 하락한 것이다. 지난 9월 29억3000만원에 주인이 바뀐 잠원동 '래미안신반포팰리스' 전용84㎡도 지난달 28억4000만원에 매매돼 두달 사이에 9000만원 떨어졌다.\n",
        "송파구에서도 잠실동 '잠실엘스' 전용 84.88㎡가 지난달 27일 22억9000만원에 거래돼 2021년 9월 기록했던 최고가(27억원)보다 4억1000만원 하락했다. 이뿐만이 아니다. 부산 등 지방 랜드마크 아파트에서도 수억 하락한 거래가 포착되고 있다.\n",
        "아실에 따르면 최근 일주일 동안 실거래 가격이 최고가 대비 3억원 이상 내린 단지는 전국에서 20여곳에 이른다. 2억원 이상 하락한 단지를 포함하면 60곳이 넘었다. 한국부동산원 통계를 보면 서울 강남 3구를 중심으로 대도시지역에서 하락폭이 커지고 있다.\n",
        "박원갑 KB국민은행 부동산 수석전문위원은 “올해 강남권 회복세가 다른 지역에 비해서 가파르게 진행되면서 현재는 가격 메리트가 떨어진 점이 가장 결정적이라고 볼 수 있다”고 분석했다.\n",
        "매매시장도 빠르게 냉각되고 있다. 직방에 따르면 올해 전국 아파트 매매거래 회전율은 3.04%로 2022년 2.28%에 비해 0.76%p 상승했다. 실거래 신고가 최초 도입된 2006년 이후 장기 시계열을 살펴보면 지난해(2.28%) 이후 역대 두번째로 낮은 수치를 기록했다. 아파트 매매 거래 회전율이 5%이하를 기록한 경우는 2022년과 2023년 뿐이다. 회전율은 아파트 재고 가구수 대비 실제 매매 거래된 해당 아파트 거래량 비율을 뜻한다. .\n",
        "함영진 직방 빅데이터랩장은 “올 상반기 반짝 회복된 일부지역의 아파트 매매거래 회복흐름이 하반기 들어 다시 주춤한 모습이다”라며 \"고금리에 경기둔화와 주택가격 부담에 대한 우려로 위축세가 뚜렷하다”고 말했다.\n",
        "\"\"\"\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts([text])\n",
        "encoded = tok.texts_to_sequences([text])[0]\n",
        "print(encoded)\n",
        "print(tok.word_index)  # {'눈이': 1, '거리에': 2, '하얀': 3, '쌓이고': 4, '있다': 5, '그의': 6, '밝게': 7 ...\n",
        "\n",
        "vocab_size = len(tok.word_index) + 1  # 0이 앞에 붙어있기 때문에 1을 더해줘야함"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터\n",
        "sequences = list()\n",
        "for line in text.split('\\n'):  # 문장 토큰화, 엔터를 기준으로 잘랐기에 line은 3개\n",
        "  enco = tok.texts_to_sequences([line])[0]\n",
        "  # print(enco)  # [2, 3, 1, 4, 5] [6, 1, 7, 8] [1, 9, 10, 11, 12, 1, 13]\n",
        "  for i in range(1, len(enco)):\n",
        "    sequ = enco[:i + 1]\n",
        "    # print(sequ)\n",
        "    sequences.append(sequ)\n",
        "\n",
        "print('학습에 참여할  샘플 수 : %d'%len(sequences))\n",
        "print(sequences)\n",
        "max_len = max(len(i) for i in sequences)  # 요소의 개수가 제일 긴게 7\n",
        "\n",
        "psequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(psequences)\n",
        "\n",
        "x = psequences[:, :-1]\n",
        "y = psequences[:, -1]\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(y[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfLlcj9-sv5-",
        "outputId": "d122f947-d92f-4478-ebcd-210c91652caa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습에 참여할  샘플 수 : 276\n",
            "[[1, 37], [1, 37, 38], [1, 37, 38, 39], [1, 37, 38, 39, 40], [1, 37, 38, 39, 40, 41], [1, 37, 38, 39, 40, 41, 42], [1, 37, 38, 39, 40, 41, 42, 43], [1, 37, 38, 39, 40, 41, 42, 43, 44], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3, 6], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3, 6, 47], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3, 6, 47, 48], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3, 6, 47, 48, 49], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3, 6, 47, 48, 49, 2], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3, 6, 47, 48, 49, 2, 50], [1, 37, 38, 39, 40, 41, 42, 43, 44, 45, 11, 12, 13, 46, 3, 6, 47, 48, 49, 2, 50, 51], [52, 53], [52, 53, 54], [52, 53, 54, 55], [52, 53, 54, 55, 14], [52, 53, 54, 55, 14, 7], [52, 53, 54, 55, 14, 7, 8], [52, 53, 54, 55, 14, 7, 8, 6], [52, 53, 54, 55, 14, 7, 8, 6, 15], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74, 75], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74, 75, 5], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74, 75, 5, 76], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74, 75, 5, 76, 77], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74, 75, 5, 76, 77, 78], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74, 75, 5, 76, 77, 78, 79], [52, 53, 54, 55, 14, 7, 8, 6, 15, 56, 4, 9, 16, 57, 2, 17, 18, 58, 59, 60, 61, 62, 1, 19, 63, 64, 10, 65, 66, 20, 21, 22, 67, 23, 68, 69, 4, 70, 24, 71, 72, 73, 74, 75, 5, 76, 77, 78, 79, 25], [80, 81], [80, 81, 82], [80, 81, 82, 83], [80, 81, 82, 83, 26], [80, 81, 82, 83, 26, 84], [80, 81, 82, 83, 26, 84, 85], [80, 81, 82, 83, 26, 84, 85, 86], [80, 81, 82, 83, 26, 84, 85, 86, 8], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97, 10], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97, 10, 98], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97, 10, 98, 99], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97, 10, 98, 99, 100], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97, 10, 98, 99, 100, 101], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97, 10, 98, 99, 100, 101, 102], [80, 81, 82, 83, 26, 84, 85, 86, 8, 87, 20, 21, 27, 5, 88, 89, 28, 90, 2, 91, 27, 5, 92, 93, 94, 95, 96, 97, 10, 98, 99, 100, 101, 102, 25], [103, 104], [103, 104, 105], [103, 104, 105, 19], [103, 104, 105, 19, 106], [103, 104, 105, 19, 106, 107], [103, 104, 105, 19, 106, 107, 10], [103, 104, 105, 19, 106, 107, 10, 108], [103, 104, 105, 19, 106, 107, 10, 108, 109], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119, 120], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119, 120, 121], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119, 120, 121, 11], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119, 120, 121, 11, 2], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119, 120, 121, 11, 2, 13], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119, 120, 121, 11, 2, 13, 122], [103, 104, 105, 19, 106, 107, 10, 108, 109, 110, 111, 5, 112, 4, 113, 24, 114, 115, 116, 26, 117, 118, 119, 120, 121, 11, 2, 13, 122, 3], [14, 7], [14, 7, 8], [14, 7, 8, 6], [14, 7, 8, 6, 15], [14, 7, 8, 6, 15, 29], [14, 7, 8, 6, 15, 29, 123], [14, 7, 8, 6, 15, 29, 123, 4], [14, 7, 8, 6, 15, 29, 123, 4, 9], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18, 137], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18, 137, 138], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18, 137, 138, 139], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18, 137, 138, 139, 140], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18, 137, 138, 139, 140, 141], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18, 137, 138, 139, 140, 141, 142], [14, 7, 8, 6, 15, 29, 123, 4, 9, 124, 30, 125, 17, 126, 127, 128, 129, 30, 2, 130, 131, 132, 133, 134, 135, 136, 18, 137, 138, 139, 140, 141, 142, 3], [143, 144], [143, 144, 145], [143, 144, 145, 146], [143, 144, 145, 146, 147], [143, 144, 145, 146, 147, 148], [143, 144, 145, 146, 147, 148, 149], [143, 144, 145, 146, 147, 148, 149, 150], [143, 144, 145, 146, 147, 148, 149, 150, 151], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12, 158], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12, 158, 16], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12, 158, 16, 159], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12, 158, 16, 159, 160], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12, 158, 16, 159, 160, 161], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12, 158, 16, 159, 160, 161, 162], [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 12, 158, 16, 159, 160, 161, 162, 163], [164, 165], [164, 165, 166], [164, 165, 166, 3], [164, 165, 166, 3, 167], [164, 165, 166, 3, 167, 7], [164, 165, 166, 3, 167, 7, 168], [164, 165, 166, 3, 167, 7, 168, 169], [164, 165, 166, 3, 167, 7, 168, 169, 1], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202, 36], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202, 36, 23], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202, 36, 23, 203], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202, 36, 23, 203, 1], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202, 36, 23, 203, 1, 204], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202, 36, 23, 203, 1, 204, 205], [164, 165, 166, 3, 167, 7, 168, 169, 1, 31, 32, 170, 171, 172, 22, 33, 34, 173, 28, 174, 175, 176, 177, 29, 178, 179, 180, 181, 35, 182, 183, 184, 185, 33, 34, 35, 186, 187, 188, 189, 190, 1, 36, 191, 192, 193, 194, 195, 196, 197, 198, 199, 32, 1, 200, 201, 9, 202, 36, 23, 203, 1, 204, 205, 206], [207, 208], [207, 208, 209], [207, 208, 209, 210], [207, 208, 209, 210, 211], [207, 208, 209, 210, 211, 212], [207, 208, 209, 210, 211, 212, 213], [207, 208, 209, 210, 211, 212, 213, 214], [207, 208, 209, 210, 211, 212, 213, 214, 1], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222, 223], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228], [207, 208, 209, 210, 211, 212, 213, 214, 1, 31, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229]]\n",
            "[[  0   0   0 ...   0   1  37]\n",
            " [  0   0   0 ...   1  37  38]\n",
            " [  0   0   0 ...  37  38  39]\n",
            " ...\n",
            " [  0   0   0 ... 225 226 227]\n",
            " [  0   0   0 ... 226 227 228]\n",
            " [  0   0   0 ... 227 228 229]]\n",
            "[[  0   0   0 ...   0   0   1]\n",
            " [  0   0   0 ...   0   1  37]\n",
            " [  0   0   0 ...   1  37  38]\n",
            " ...\n",
            " [  0   0   0 ... 224 225 226]\n",
            " [  0   0   0 ... 225 226 227]\n",
            " [  0   0   0 ... 226 227 228]]\n",
            "[ 37  38  39  40  41  42  43  44  45  11  12  13  46   3   6  47  48  49\n",
            "   2  50  51  53  54  55  14   7   8   6  15  56   4   9  16  57   2  17\n",
            "  18  58  59  60  61  62   1  19  63  64  10  65  66  20  21  22  67  23\n",
            "  68  69   4  70  24  71  72  73  74  75   5  76  77  78  79  25  81  82\n",
            "  83  26  84  85  86   8  87  20  21  27   5  88  89  28  90   2  91  27\n",
            "   5  92  93  94  95  96  97  10  98  99 100 101 102  25 104 105  19 106\n",
            " 107  10 108 109 110 111   5 112   4 113  24 114 115 116  26 117 118 119\n",
            " 120 121  11   2  13 122   3   7   8   6  15  29 123   4   9 124  30 125\n",
            "  17 126 127 128 129  30   2 130 131 132 133 134 135 136  18 137 138 139\n",
            " 140 141 142   3 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            "  12 158  16 159 160 161 162 163 165 166   3 167   7 168 169   1  31  32\n",
            " 170 171 172  22  33  34 173  28 174 175 176 177  29 178 179 180 181  35\n",
            " 182 183 184 185  33  34  35 186 187 188 189 190   1  36 191 192 193 194\n",
            " 195 196 197 198 199  32   1 200 201   9 202  36  23 203   1 204 205 206\n",
            " 208 209 210 211 212 213 214   1  31 215 216 217 218 219 220 221 222 223\n",
            " 224 225 226 227 228 229]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 32, input_length=max_len - 1))\n",
        "model.add(LSTM(32, activation='tanh'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.fit(x, y, epochs=200, verbose=2)\n",
        "print('model.evaluate : ', model.evaluate(x,y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVvkt4P7yscf",
        "outputId": "05594115-f499-482e-efcf-2483bff583c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 64, 32)            7360      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 230)               7590      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25382 (99.15 KB)\n",
            "Trainable params: 25382 (99.15 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "9/9 - 4s - loss: 5.4395 - 4s/epoch - 494ms/step\n",
            "Epoch 2/200\n",
            "9/9 - 0s - loss: 5.4359 - 430ms/epoch - 48ms/step\n",
            "Epoch 3/200\n",
            "9/9 - 0s - loss: 5.4325 - 393ms/epoch - 44ms/step\n",
            "Epoch 4/200\n",
            "9/9 - 0s - loss: 5.4271 - 416ms/epoch - 46ms/step\n",
            "Epoch 5/200\n",
            "9/9 - 0s - loss: 5.4149 - 397ms/epoch - 44ms/step\n",
            "Epoch 6/200\n",
            "9/9 - 0s - loss: 5.3784 - 435ms/epoch - 48ms/step\n",
            "Epoch 7/200\n",
            "9/9 - 0s - loss: 5.3290 - 412ms/epoch - 46ms/step\n",
            "Epoch 8/200\n",
            "9/9 - 0s - loss: 5.2511 - 474ms/epoch - 53ms/step\n",
            "Epoch 9/200\n",
            "9/9 - 0s - loss: 5.1815 - 466ms/epoch - 52ms/step\n",
            "Epoch 10/200\n",
            "9/9 - 1s - loss: 5.1044 - 508ms/epoch - 56ms/step\n",
            "Epoch 11/200\n",
            "9/9 - 0s - loss: 5.0255 - 422ms/epoch - 47ms/step\n",
            "Epoch 12/200\n",
            "9/9 - 0s - loss: 4.9405 - 432ms/epoch - 48ms/step\n",
            "Epoch 13/200\n",
            "9/9 - 0s - loss: 4.8715 - 405ms/epoch - 45ms/step\n",
            "Epoch 14/200\n",
            "9/9 - 0s - loss: 4.7866 - 434ms/epoch - 48ms/step\n",
            "Epoch 15/200\n",
            "9/9 - 1s - loss: 4.7110 - 505ms/epoch - 56ms/step\n",
            "Epoch 16/200\n",
            "9/9 - 0s - loss: 4.6337 - 478ms/epoch - 53ms/step\n",
            "Epoch 17/200\n",
            "9/9 - 0s - loss: 4.5648 - 437ms/epoch - 49ms/step\n",
            "Epoch 18/200\n",
            "9/9 - 0s - loss: 4.4721 - 456ms/epoch - 51ms/step\n",
            "Epoch 19/200\n",
            "9/9 - 0s - loss: 4.3964 - 467ms/epoch - 52ms/step\n",
            "Epoch 20/200\n",
            "9/9 - 0s - loss: 4.3082 - 452ms/epoch - 50ms/step\n",
            "Epoch 21/200\n",
            "9/9 - 0s - loss: 4.1975 - 404ms/epoch - 45ms/step\n",
            "Epoch 22/200\n",
            "9/9 - 1s - loss: 4.1005 - 555ms/epoch - 62ms/step\n",
            "Epoch 23/200\n",
            "9/9 - 0s - loss: 3.9910 - 445ms/epoch - 49ms/step\n",
            "Epoch 24/200\n",
            "9/9 - 0s - loss: 3.8980 - 233ms/epoch - 26ms/step\n",
            "Epoch 25/200\n",
            "9/9 - 0s - loss: 3.7868 - 245ms/epoch - 27ms/step\n",
            "Epoch 26/200\n",
            "9/9 - 0s - loss: 3.6902 - 231ms/epoch - 26ms/step\n",
            "Epoch 27/200\n",
            "9/9 - 0s - loss: 3.6177 - 240ms/epoch - 27ms/step\n",
            "Epoch 28/200\n",
            "9/9 - 0s - loss: 3.5169 - 231ms/epoch - 26ms/step\n",
            "Epoch 29/200\n",
            "9/9 - 0s - loss: 3.4245 - 251ms/epoch - 28ms/step\n",
            "Epoch 30/200\n",
            "9/9 - 0s - loss: 3.3537 - 245ms/epoch - 27ms/step\n",
            "Epoch 31/200\n",
            "9/9 - 0s - loss: 3.2681 - 242ms/epoch - 27ms/step\n",
            "Epoch 32/200\n",
            "9/9 - 0s - loss: 3.1936 - 232ms/epoch - 26ms/step\n",
            "Epoch 33/200\n",
            "9/9 - 0s - loss: 3.1313 - 254ms/epoch - 28ms/step\n",
            "Epoch 34/200\n",
            "9/9 - 0s - loss: 3.0656 - 234ms/epoch - 26ms/step\n",
            "Epoch 35/200\n",
            "9/9 - 0s - loss: 3.0072 - 239ms/epoch - 27ms/step\n",
            "Epoch 36/200\n",
            "9/9 - 0s - loss: 2.9543 - 242ms/epoch - 27ms/step\n",
            "Epoch 37/200\n",
            "9/9 - 0s - loss: 2.8919 - 279ms/epoch - 31ms/step\n",
            "Epoch 38/200\n",
            "9/9 - 0s - loss: 2.8310 - 382ms/epoch - 42ms/step\n",
            "Epoch 39/200\n",
            "9/9 - 0s - loss: 2.7974 - 388ms/epoch - 43ms/step\n",
            "Epoch 40/200\n",
            "9/9 - 0s - loss: 2.7531 - 411ms/epoch - 46ms/step\n",
            "Epoch 41/200\n",
            "9/9 - 0s - loss: 2.7011 - 403ms/epoch - 45ms/step\n",
            "Epoch 42/200\n",
            "9/9 - 0s - loss: 2.6553 - 399ms/epoch - 44ms/step\n",
            "Epoch 43/200\n",
            "9/9 - 0s - loss: 2.6026 - 401ms/epoch - 45ms/step\n",
            "Epoch 44/200\n",
            "9/9 - 0s - loss: 2.5588 - 407ms/epoch - 45ms/step\n",
            "Epoch 45/200\n",
            "9/9 - 0s - loss: 2.5200 - 377ms/epoch - 42ms/step\n",
            "Epoch 46/200\n",
            "9/9 - 0s - loss: 2.4754 - 372ms/epoch - 41ms/step\n",
            "Epoch 47/200\n",
            "9/9 - 0s - loss: 2.4431 - 361ms/epoch - 40ms/step\n",
            "Epoch 48/200\n",
            "9/9 - 0s - loss: 2.4113 - 365ms/epoch - 41ms/step\n",
            "Epoch 49/200\n",
            "9/9 - 1s - loss: 2.3793 - 526ms/epoch - 58ms/step\n",
            "Epoch 50/200\n",
            "9/9 - 0s - loss: 2.3355 - 409ms/epoch - 45ms/step\n",
            "Epoch 51/200\n",
            "9/9 - 0s - loss: 2.3179 - 329ms/epoch - 37ms/step\n",
            "Epoch 52/200\n",
            "9/9 - 0s - loss: 2.2683 - 235ms/epoch - 26ms/step\n",
            "Epoch 53/200\n",
            "9/9 - 0s - loss: 2.2361 - 233ms/epoch - 26ms/step\n",
            "Epoch 54/200\n",
            "9/9 - 0s - loss: 2.1852 - 249ms/epoch - 28ms/step\n",
            "Epoch 55/200\n",
            "9/9 - 0s - loss: 2.1624 - 238ms/epoch - 26ms/step\n",
            "Epoch 56/200\n",
            "9/9 - 0s - loss: 2.1274 - 232ms/epoch - 26ms/step\n",
            "Epoch 57/200\n",
            "9/9 - 0s - loss: 2.0992 - 231ms/epoch - 26ms/step\n",
            "Epoch 58/200\n",
            "9/9 - 0s - loss: 2.0746 - 235ms/epoch - 26ms/step\n",
            "Epoch 59/200\n",
            "9/9 - 0s - loss: 2.0268 - 236ms/epoch - 26ms/step\n",
            "Epoch 60/200\n",
            "9/9 - 0s - loss: 2.0166 - 227ms/epoch - 25ms/step\n",
            "Epoch 61/200\n",
            "9/9 - 0s - loss: 1.9753 - 247ms/epoch - 27ms/step\n",
            "Epoch 62/200\n",
            "9/9 - 0s - loss: 1.9505 - 235ms/epoch - 26ms/step\n",
            "Epoch 63/200\n",
            "9/9 - 0s - loss: 1.9203 - 331ms/epoch - 37ms/step\n",
            "Epoch 64/200\n",
            "9/9 - 0s - loss: 1.8781 - 394ms/epoch - 44ms/step\n",
            "Epoch 65/200\n",
            "9/9 - 0s - loss: 1.8559 - 375ms/epoch - 42ms/step\n",
            "Epoch 66/200\n",
            "9/9 - 0s - loss: 1.8144 - 377ms/epoch - 42ms/step\n",
            "Epoch 67/200\n",
            "9/9 - 0s - loss: 1.7744 - 333ms/epoch - 37ms/step\n",
            "Epoch 68/200\n",
            "9/9 - 0s - loss: 1.7411 - 231ms/epoch - 26ms/step\n",
            "Epoch 69/200\n",
            "9/9 - 0s - loss: 1.7150 - 247ms/epoch - 27ms/step\n",
            "Epoch 70/200\n",
            "9/9 - 0s - loss: 1.6775 - 231ms/epoch - 26ms/step\n",
            "Epoch 71/200\n",
            "9/9 - 0s - loss: 1.6539 - 231ms/epoch - 26ms/step\n",
            "Epoch 72/200\n",
            "9/9 - 0s - loss: 1.6273 - 240ms/epoch - 27ms/step\n",
            "Epoch 73/200\n",
            "9/9 - 0s - loss: 1.5981 - 254ms/epoch - 28ms/step\n",
            "Epoch 74/200\n",
            "9/9 - 0s - loss: 1.5967 - 236ms/epoch - 26ms/step\n",
            "Epoch 75/200\n",
            "9/9 - 0s - loss: 1.5724 - 231ms/epoch - 26ms/step\n",
            "Epoch 76/200\n",
            "9/9 - 0s - loss: 1.5461 - 233ms/epoch - 26ms/step\n",
            "Epoch 77/200\n",
            "9/9 - 0s - loss: 1.5104 - 245ms/epoch - 27ms/step\n",
            "Epoch 78/200\n",
            "9/9 - 0s - loss: 1.4800 - 229ms/epoch - 25ms/step\n",
            "Epoch 79/200\n",
            "9/9 - 0s - loss: 1.4427 - 240ms/epoch - 27ms/step\n",
            "Epoch 80/200\n",
            "9/9 - 0s - loss: 1.4224 - 232ms/epoch - 26ms/step\n",
            "Epoch 81/200\n",
            "9/9 - 0s - loss: 1.4000 - 233ms/epoch - 26ms/step\n",
            "Epoch 82/200\n",
            "9/9 - 0s - loss: 1.3731 - 245ms/epoch - 27ms/step\n",
            "Epoch 83/200\n",
            "9/9 - 0s - loss: 1.3478 - 239ms/epoch - 27ms/step\n",
            "Epoch 84/200\n",
            "9/9 - 0s - loss: 1.3291 - 231ms/epoch - 26ms/step\n",
            "Epoch 85/200\n",
            "9/9 - 0s - loss: 1.3073 - 230ms/epoch - 26ms/step\n",
            "Epoch 86/200\n",
            "9/9 - 0s - loss: 1.2897 - 256ms/epoch - 28ms/step\n",
            "Epoch 87/200\n",
            "9/9 - 0s - loss: 1.2742 - 343ms/epoch - 38ms/step\n",
            "Epoch 88/200\n",
            "9/9 - 0s - loss: 1.2300 - 378ms/epoch - 42ms/step\n",
            "Epoch 89/200\n",
            "9/9 - 0s - loss: 1.2080 - 414ms/epoch - 46ms/step\n",
            "Epoch 90/200\n",
            "9/9 - 0s - loss: 1.1810 - 398ms/epoch - 44ms/step\n",
            "Epoch 91/200\n",
            "9/9 - 0s - loss: 1.1671 - 366ms/epoch - 41ms/step\n",
            "Epoch 92/200\n",
            "9/9 - 0s - loss: 1.1366 - 376ms/epoch - 42ms/step\n",
            "Epoch 93/200\n",
            "9/9 - 0s - loss: 1.1199 - 394ms/epoch - 44ms/step\n",
            "Epoch 94/200\n",
            "9/9 - 0s - loss: 1.0893 - 390ms/epoch - 43ms/step\n",
            "Epoch 95/200\n",
            "9/9 - 0s - loss: 1.0595 - 389ms/epoch - 43ms/step\n",
            "Epoch 96/200\n",
            "9/9 - 0s - loss: 1.0443 - 384ms/epoch - 43ms/step\n",
            "Epoch 97/200\n",
            "9/9 - 0s - loss: 1.0287 - 382ms/epoch - 42ms/step\n",
            "Epoch 98/200\n",
            "9/9 - 0s - loss: 1.0054 - 379ms/epoch - 42ms/step\n",
            "Epoch 99/200\n",
            "9/9 - 0s - loss: 0.9831 - 284ms/epoch - 32ms/step\n",
            "Epoch 100/200\n",
            "9/9 - 0s - loss: 0.9761 - 253ms/epoch - 28ms/step\n",
            "Epoch 101/200\n",
            "9/9 - 0s - loss: 0.9482 - 239ms/epoch - 27ms/step\n",
            "Epoch 102/200\n",
            "9/9 - 0s - loss: 0.9275 - 236ms/epoch - 26ms/step\n",
            "Epoch 103/200\n",
            "9/9 - 0s - loss: 0.9118 - 237ms/epoch - 26ms/step\n",
            "Epoch 104/200\n",
            "9/9 - 0s - loss: 0.8891 - 241ms/epoch - 27ms/step\n",
            "Epoch 105/200\n",
            "9/9 - 0s - loss: 0.8661 - 232ms/epoch - 26ms/step\n",
            "Epoch 106/200\n",
            "9/9 - 0s - loss: 0.8564 - 240ms/epoch - 27ms/step\n",
            "Epoch 107/200\n",
            "9/9 - 0s - loss: 0.8518 - 239ms/epoch - 27ms/step\n",
            "Epoch 108/200\n",
            "9/9 - 0s - loss: 0.8041 - 241ms/epoch - 27ms/step\n",
            "Epoch 109/200\n",
            "9/9 - 0s - loss: 0.8145 - 236ms/epoch - 26ms/step\n",
            "Epoch 110/200\n",
            "9/9 - 0s - loss: 0.7834 - 235ms/epoch - 26ms/step\n",
            "Epoch 111/200\n",
            "9/9 - 0s - loss: 0.7785 - 230ms/epoch - 26ms/step\n",
            "Epoch 112/200\n",
            "9/9 - 0s - loss: 0.7709 - 234ms/epoch - 26ms/step\n",
            "Epoch 113/200\n",
            "9/9 - 0s - loss: 0.7301 - 238ms/epoch - 26ms/step\n",
            "Epoch 114/200\n",
            "9/9 - 0s - loss: 0.7156 - 234ms/epoch - 26ms/step\n",
            "Epoch 115/200\n",
            "9/9 - 0s - loss: 0.6967 - 229ms/epoch - 25ms/step\n",
            "Epoch 116/200\n",
            "9/9 - 0s - loss: 0.6941 - 239ms/epoch - 27ms/step\n",
            "Epoch 117/200\n",
            "9/9 - 0s - loss: 0.6640 - 244ms/epoch - 27ms/step\n",
            "Epoch 118/200\n",
            "9/9 - 0s - loss: 0.6662 - 237ms/epoch - 26ms/step\n",
            "Epoch 119/200\n",
            "9/9 - 0s - loss: 0.6538 - 236ms/epoch - 26ms/step\n",
            "Epoch 120/200\n",
            "9/9 - 0s - loss: 0.6363 - 231ms/epoch - 26ms/step\n",
            "Epoch 121/200\n",
            "9/9 - 0s - loss: 0.6120 - 244ms/epoch - 27ms/step\n",
            "Epoch 122/200\n",
            "9/9 - 0s - loss: 0.6086 - 228ms/epoch - 25ms/step\n",
            "Epoch 123/200\n",
            "9/9 - 0s - loss: 0.5998 - 221ms/epoch - 25ms/step\n",
            "Epoch 124/200\n",
            "9/9 - 0s - loss: 0.5825 - 253ms/epoch - 28ms/step\n",
            "Epoch 125/200\n",
            "9/9 - 0s - loss: 0.5545 - 229ms/epoch - 25ms/step\n",
            "Epoch 126/200\n",
            "9/9 - 0s - loss: 0.5530 - 233ms/epoch - 26ms/step\n",
            "Epoch 127/200\n",
            "9/9 - 0s - loss: 0.5188 - 231ms/epoch - 26ms/step\n",
            "Epoch 128/200\n",
            "9/9 - 0s - loss: 0.5130 - 230ms/epoch - 26ms/step\n",
            "Epoch 129/200\n",
            "9/9 - 0s - loss: 0.5084 - 232ms/epoch - 26ms/step\n",
            "Epoch 130/200\n",
            "9/9 - 0s - loss: 0.4912 - 240ms/epoch - 27ms/step\n",
            "Epoch 131/200\n",
            "9/9 - 0s - loss: 0.4703 - 228ms/epoch - 25ms/step\n",
            "Epoch 132/200\n",
            "9/9 - 0s - loss: 0.4622 - 241ms/epoch - 27ms/step\n",
            "Epoch 133/200\n",
            "9/9 - 0s - loss: 0.4480 - 235ms/epoch - 26ms/step\n",
            "Epoch 134/200\n",
            "9/9 - 0s - loss: 0.4419 - 233ms/epoch - 26ms/step\n",
            "Epoch 135/200\n",
            "9/9 - 0s - loss: 0.4378 - 228ms/epoch - 25ms/step\n",
            "Epoch 136/200\n",
            "9/9 - 0s - loss: 0.4166 - 224ms/epoch - 25ms/step\n",
            "Epoch 137/200\n",
            "9/9 - 0s - loss: 0.4137 - 243ms/epoch - 27ms/step\n",
            "Epoch 138/200\n",
            "9/9 - 0s - loss: 0.3981 - 227ms/epoch - 25ms/step\n",
            "Epoch 139/200\n",
            "9/9 - 0s - loss: 0.3923 - 246ms/epoch - 27ms/step\n",
            "Epoch 140/200\n",
            "9/9 - 0s - loss: 0.3726 - 224ms/epoch - 25ms/step\n",
            "Epoch 141/200\n",
            "9/9 - 0s - loss: 0.3659 - 372ms/epoch - 41ms/step\n",
            "Epoch 142/200\n",
            "9/9 - 0s - loss: 0.3590 - 392ms/epoch - 44ms/step\n",
            "Epoch 143/200\n",
            "9/9 - 0s - loss: 0.3475 - 401ms/epoch - 45ms/step\n",
            "Epoch 144/200\n",
            "9/9 - 0s - loss: 0.3486 - 389ms/epoch - 43ms/step\n",
            "Epoch 145/200\n",
            "9/9 - 0s - loss: 0.3375 - 370ms/epoch - 41ms/step\n",
            "Epoch 146/200\n",
            "9/9 - 0s - loss: 0.3306 - 364ms/epoch - 40ms/step\n",
            "Epoch 147/200\n",
            "9/9 - 0s - loss: 0.3166 - 396ms/epoch - 44ms/step\n",
            "Epoch 148/200\n",
            "9/9 - 0s - loss: 0.3084 - 416ms/epoch - 46ms/step\n",
            "Epoch 149/200\n",
            "9/9 - 0s - loss: 0.2965 - 371ms/epoch - 41ms/step\n",
            "Epoch 150/200\n",
            "9/9 - 0s - loss: 0.2849 - 372ms/epoch - 41ms/step\n",
            "Epoch 151/200\n",
            "9/9 - 0s - loss: 0.2887 - 383ms/epoch - 43ms/step\n",
            "Epoch 152/200\n",
            "9/9 - 0s - loss: 0.2812 - 378ms/epoch - 42ms/step\n",
            "Epoch 153/200\n",
            "9/9 - 0s - loss: 0.2684 - 282ms/epoch - 31ms/step\n",
            "Epoch 154/200\n",
            "9/9 - 0s - loss: 0.2733 - 238ms/epoch - 26ms/step\n",
            "Epoch 155/200\n",
            "9/9 - 0s - loss: 0.2633 - 241ms/epoch - 27ms/step\n",
            "Epoch 156/200\n",
            "9/9 - 0s - loss: 0.2527 - 237ms/epoch - 26ms/step\n",
            "Epoch 157/200\n",
            "9/9 - 0s - loss: 0.2475 - 253ms/epoch - 28ms/step\n",
            "Epoch 158/200\n",
            "9/9 - 0s - loss: 0.2419 - 233ms/epoch - 26ms/step\n",
            "Epoch 159/200\n",
            "9/9 - 0s - loss: 0.2277 - 238ms/epoch - 26ms/step\n",
            "Epoch 160/200\n",
            "9/9 - 0s - loss: 0.2241 - 240ms/epoch - 27ms/step\n",
            "Epoch 161/200\n",
            "9/9 - 0s - loss: 0.2222 - 250ms/epoch - 28ms/step\n",
            "Epoch 162/200\n",
            "9/9 - 0s - loss: 0.2103 - 229ms/epoch - 25ms/step\n",
            "Epoch 163/200\n",
            "9/9 - 0s - loss: 0.2059 - 238ms/epoch - 26ms/step\n",
            "Epoch 164/200\n",
            "9/9 - 0s - loss: 0.2003 - 238ms/epoch - 26ms/step\n",
            "Epoch 165/200\n",
            "9/9 - 0s - loss: 0.1961 - 237ms/epoch - 26ms/step\n",
            "Epoch 166/200\n",
            "9/9 - 0s - loss: 0.1968 - 247ms/epoch - 27ms/step\n",
            "Epoch 167/200\n",
            "9/9 - 0s - loss: 0.2033 - 241ms/epoch - 27ms/step\n",
            "Epoch 168/200\n",
            "9/9 - 0s - loss: 0.1866 - 239ms/epoch - 27ms/step\n",
            "Epoch 169/200\n",
            "9/9 - 0s - loss: 0.1795 - 235ms/epoch - 26ms/step\n",
            "Epoch 170/200\n",
            "9/9 - 0s - loss: 0.1777 - 249ms/epoch - 28ms/step\n",
            "Epoch 171/200\n",
            "9/9 - 0s - loss: 0.1717 - 237ms/epoch - 26ms/step\n",
            "Epoch 172/200\n",
            "9/9 - 0s - loss: 0.1686 - 227ms/epoch - 25ms/step\n",
            "Epoch 173/200\n",
            "9/9 - 0s - loss: 0.1588 - 232ms/epoch - 26ms/step\n",
            "Epoch 174/200\n",
            "9/9 - 0s - loss: 0.1555 - 242ms/epoch - 27ms/step\n",
            "Epoch 175/200\n",
            "9/9 - 0s - loss: 0.1478 - 240ms/epoch - 27ms/step\n",
            "Epoch 176/200\n",
            "9/9 - 0s - loss: 0.1495 - 234ms/epoch - 26ms/step\n",
            "Epoch 177/200\n",
            "9/9 - 0s - loss: 0.1440 - 250ms/epoch - 28ms/step\n",
            "Epoch 178/200\n",
            "9/9 - 0s - loss: 0.1393 - 243ms/epoch - 27ms/step\n",
            "Epoch 179/200\n",
            "9/9 - 0s - loss: 0.1368 - 243ms/epoch - 27ms/step\n",
            "Epoch 180/200\n",
            "9/9 - 0s - loss: 0.1333 - 225ms/epoch - 25ms/step\n",
            "Epoch 181/200\n",
            "9/9 - 0s - loss: 0.1321 - 240ms/epoch - 27ms/step\n",
            "Epoch 182/200\n",
            "9/9 - 0s - loss: 0.1300 - 240ms/epoch - 27ms/step\n",
            "Epoch 183/200\n",
            "9/9 - 0s - loss: 0.1257 - 240ms/epoch - 27ms/step\n",
            "Epoch 184/200\n",
            "9/9 - 0s - loss: 0.1233 - 235ms/epoch - 26ms/step\n",
            "Epoch 185/200\n",
            "9/9 - 0s - loss: 0.1231 - 234ms/epoch - 26ms/step\n",
            "Epoch 186/200\n",
            "9/9 - 0s - loss: 0.1197 - 224ms/epoch - 25ms/step\n",
            "Epoch 187/200\n",
            "9/9 - 0s - loss: 0.1152 - 237ms/epoch - 26ms/step\n",
            "Epoch 188/200\n",
            "9/9 - 0s - loss: 0.1122 - 251ms/epoch - 28ms/step\n",
            "Epoch 189/200\n",
            "9/9 - 0s - loss: 0.1094 - 237ms/epoch - 26ms/step\n",
            "Epoch 190/200\n",
            "9/9 - 0s - loss: 0.1071 - 234ms/epoch - 26ms/step\n",
            "Epoch 191/200\n",
            "9/9 - 0s - loss: 0.1039 - 239ms/epoch - 27ms/step\n",
            "Epoch 192/200\n",
            "9/9 - 0s - loss: 0.1002 - 240ms/epoch - 27ms/step\n",
            "Epoch 193/200\n",
            "9/9 - 0s - loss: 0.0950 - 233ms/epoch - 26ms/step\n",
            "Epoch 194/200\n",
            "9/9 - 0s - loss: 0.0939 - 296ms/epoch - 33ms/step\n",
            "Epoch 195/200\n",
            "9/9 - 0s - loss: 0.0916 - 378ms/epoch - 42ms/step\n",
            "Epoch 196/200\n",
            "9/9 - 0s - loss: 0.0915 - 385ms/epoch - 43ms/step\n",
            "Epoch 197/200\n",
            "9/9 - 0s - loss: 0.0874 - 411ms/epoch - 46ms/step\n",
            "Epoch 198/200\n",
            "9/9 - 0s - loss: 0.0859 - 391ms/epoch - 43ms/step\n",
            "Epoch 199/200\n",
            "9/9 - 0s - loss: 0.0831 - 366ms/epoch - 41ms/step\n",
            "Epoch 200/200\n",
            "9/9 - 0s - loss: 0.0804 - 378ms/epoch - 42ms/step\n",
            "9/9 [==============================] - 1s 9ms/step - loss: 0.0749\n",
            "model.evaluate :  0.0748780220746994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열 생성\n",
        "def sequence_gen_func(model, t, current_word, n):\n",
        "  init_word = current_word\n",
        "  sentence = ''\n",
        "  for _ in range(n):\n",
        "    encoded = t.texts_to_sequences([current_word])[0]  # 매트릭스를 벡터로 바꿈\n",
        "    encoded = pad_sequences([encoded], maxlen=max_len - 1, padding='pre')\n",
        "    result = np.argmax(model.predict(encoded, verbose=0), axis=-1)\n",
        "\n",
        "    # 예측 단어 찾기\n",
        "    for word, index in t.word_index.items():\n",
        "      # print(word, index)\n",
        "      if index == result: # 예측한 단어의 인덱스와 동일한 단어가 있다면\n",
        "        break # 해당 단어가 예측단어 이므로 break\n",
        "\n",
        "    current_word = current_word + ' ' + word\n",
        "    sentence = sentence + ' ' + word\n",
        "  sentence = init_word + sentence\n",
        "  return sentence\n",
        "\n",
        "# print(sequence_gen_func(model, tok, '눈이', 5))  # '눈이' 뒤에 5개의 단어가 나옴\n",
        "# print(sequence_gen_func(model, tok, '그의', 3))\n",
        "# print(sequence_gen_func(model, tok, '그의', 10))\n",
        "# print(sequence_gen_func(model, tok, '거리에', 3))\n",
        "\n",
        "print(sequence_gen_func(model, tok, '매매', 5))  # '눈이' 뒤에 5개의 단어가 나옴\n",
        "print(sequence_gen_func(model, tok, '아파트', 4))\n",
        "print(sequence_gen_func(model, tok, '가격', 6))\n",
        "print(sequence_gen_func(model, tok, '거래량', 7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0THZOUf20_kL",
        "outputId": "67ec6bb0-b25c-487c-d2cb-98bed06c9bcd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "매매 국토교통부 실거래가 시스템과 아실에 따르면\n",
            "아파트 시장이 '숨고르기'에 들어간 가운데\n",
            "가격 1번지’ 서초구도 예외는 아니다 반포동 '래미안원베일리'\n",
            "거래량 들어간 가운데 부촌 1위인 서초는 물론 강남권에서도\n"
          ]
        }
      ]
    }
  ]
}