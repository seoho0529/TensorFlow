{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzfo2G5AFoAlS6pQIFIfh0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s9RmxWuzmkDt"
      },
      "outputs": [],
      "source": [
        "# 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델입니다.\n",
        "# 예를 들어 챗봇(Chatbot)과 기계 번역(Machine Translation)이 그러한 대표적인 예인데, 입력 시퀀스와 출력 시퀀스를 각각 질문과 대답으로 구성하면 챗봇으로 만들 수 있고,\n",
        "# 입력 시퀀스와 출력 시퀀스를 각각 입력 문장과 번역 문장으로 만들면 번역기로 만들 수 있습니다.\n",
        "# 그 외에도 내용 요약(Text Summarization), STT(Speech to Text) 등에서 쓰일 수 있습니다.\n",
        "\n",
        "\n",
        "# 문자 레"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import urllib3\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def download_zip(url, output_path):\n",
        "    response = requests.get(url, headers=headers, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"ZIP file downloaded to {output_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
        "\n",
        "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
        "output_path = \"fra-eng.zip\"\n",
        "download_zip(url, output_path)\n",
        "\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, output_path)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)\n",
        "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
        "del lines['lic']\n",
        "print('전체 샘플의 개수 :',len(lines))\n",
        "print(lines[:2])\n",
        "\n",
        "lines = lines.loc[:, 'src':'tar']\n",
        "lines = lines[0:60000] # 6만개만 저장\n",
        "lines.sample(10)\n",
        "\n",
        "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
        "lines.sample(10)\n",
        "\n",
        "\n",
        "# 문자 집합 구축\n",
        "src_vocab = set()\n",
        "for line in lines.src: # 1줄씩 읽음\n",
        "    for char in line: # 1개의 문자씩 읽음\n",
        "        src_vocab.add(char)\n",
        "\n",
        "tar_vocab = set()\n",
        "for line in lines.tar:\n",
        "    for char in line:\n",
        "        tar_vocab.add(char)\n",
        "\n",
        "# 문자 집합의 크기 보기\n",
        "src_vocab_size = len(src_vocab)+1\n",
        "tar_vocab_size = len(tar_vocab)+1\n",
        "print('source 문장의 char 집합 :',src_vocab_size)\n",
        "print('target 문장의 char 집합 :',tar_vocab_size)\n",
        "\n",
        "src_vocab = sorted(list(src_vocab))\n",
        "tar_vocab = sorted(list(tar_vocab))\n",
        "print(src_vocab[60:80])\n",
        "print(tar_vocab[90:104])\n",
        "\n",
        "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
        "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
        "print(src_to_index)  # {' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7,\n",
        "print(tar_to_index)  # {'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZthDOgOHsFXM",
        "outputId": "1826dd7f-28ec-4139-d925-2971a4038155"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded to fra-eng.zip\n",
            "전체 샘플의 개수 : 229803\n",
            "   src      tar\n",
            "0  Go.     Va !\n",
            "1  Go.  Marche.\n",
            "source 문장의 char 집합 : 80\n",
            "target 문장의 char 집합 : 104\n",
            "['l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é', 'ï', '’', '€']\n",
            "['ê', 'ë', 'î', 'ï', 'ô', 'ù', 'û', 'œ', '\\u2009', '‘', '’', '\\u202f', '‽']\n",
            "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, 'ï': 77, '’': 78, '€': 79}\n",
            "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Ç': 82, 'É': 83, 'Ê': 84, 'Ô': 85, 'à': 86, 'â': 87, 'ç': 88, 'è': 89, 'é': 90, 'ê': 91, 'ë': 92, 'î': 93, 'ï': 94, 'ô': 95, 'ù': 96, 'û': 97, 'œ': 98, '\\u2009': 99, '‘': 100, '’': 101, '\\u202f': 102, '‽': 103}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더의 입력이 될 영어 문장 샘플에 대해서 정수 인코딩을 수행해보고, 5개의 샘플을 출력해봅시다.\n",
        "\n",
        "encoder_input = []\n",
        "\n",
        "# 1개의 문장\n",
        "for line in lines.src:\n",
        "  encoded_line = []\n",
        "  # 각 줄에서 1개의 char\n",
        "  for char in line:\n",
        "    # 각 char을 정수로 변환\n",
        "    encoded_line.append(src_to_index[char])\n",
        "  encoder_input.append(encoded_line)\n",
        "print('source 문장의 정수 인코딩 :',encoder_input[:5])\n",
        "\n",
        "# 디코더의 입력이 될 프랑스어 데이터에 대해서 정수 인코딩\n",
        "decoder_input = []\n",
        "for line in lines.tar:\n",
        "  encoded_line = []\n",
        "  for char in line:\n",
        "    encoded_line.append(tar_to_index[char])\n",
        "  decoder_input.append(encoded_line)\n",
        "print('target 문장의 정수 인코딩 :',decoder_input[:5])\n",
        "\n",
        "# 모든 프랑스어 문장의 맨 앞에 붙어있는 '\\t'를 제거\n",
        "decoder_target = []\n",
        "for line in lines.tar:\n",
        "  timestep = 0\n",
        "  encoded_line = []\n",
        "  for char in line:\n",
        "    if timestep > 0:\n",
        "      encoded_line.append(tar_to_index[char])\n",
        "    timestep = timestep + 1\n",
        "  decoder_target.append(encoded_line)\n",
        "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JOetYgDs_x_",
        "outputId": "10036b4c-9b29-4934-cc24-01ef64d47ccb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10]]\n",
            "target 문장의 정수 인코딩 : [[1, 3, 48, 52, 3, 4, 3, 2], [1, 3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [1, 3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [1, 3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [1, 3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n",
            "target 문장 레이블의 정수 인코딩 : [[3, 48, 52, 3, 4, 3, 2], [3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩을 위해서 영어 문장과 프랑스어 문장 각각에 대해서 가장 길이가 긴 샘플의 길이를 확인\n",
        "max_src_len = max([len(line) for line in lines.src])\n",
        "max_tar_len = max([len(line) for line in lines.tar])\n",
        "print('source 문장의 최대 길이 :',max_src_len)\n",
        "print('target 문장의 최대 길이 :',max_tar_len)\n",
        "\n",
        "# 가장 긴 샘플의 길이에 맞춰서 영어 데이터의 샘플은 전부 길이가 23이 되도록 패딩하고, 프랑스어 데이터의 샘플은 전부 길이가 76이 되도록 패딩\n",
        "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n",
        "\n",
        "# 모든 값에 대해서 원-핫 인코딩을 수행\n",
        "encoder_input = to_categorical(encoder_input)\n",
        "decoder_input = to_categorical(decoder_input)\n",
        "decoder_target = to_categorical(decoder_target)\n",
        "print(encoder_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeW0jYpTvylx",
        "outputId": "c85ceb90-97d3-4b0a-8a68-947cca29bfeb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source 문장의 최대 길이 : 22\n",
            "target 문장의 최대 길이 : 76\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2seq 모델을 설계하고 교사 강요를 사용하여 훈련시키기\n",
        "# Functional api를 사용 - 입력이 두개가 가능 (Sequential은 안됨)\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
        "encoder_lstm = LSTM(units=256, return_state=True) # return_state=True로 설정합니다. 인코더에 입력을 넣으면 내부 상태를 리턴\n",
        "\n",
        "# encoder_outputs은 여기서는 불필요\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)  # state_h : 히든\n",
        "\n",
        "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\n",
        "encoder_states = [state_h, state_c]  # 얘가 context vector(문맥 벡터) - 학습할때 여기까지 작업이 이루어짐\n",
        "\n",
        "# 이 두 가지 상태를 encoder_states에 저장합니다. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달합니다.\n",
        "# 이것이 앞서 배운 컨텍스트 벡터입니다.\n",
        "\n",
        "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
        "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
        "\n",
        "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "print(model.summary())\n",
        "# model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uplapH6EyQDL",
        "outputId": "3a72fe80-2245-46d2-cd66-3c592b54561f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None, 80)]           0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None, 104)]          0         []                            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 256),                345088    ['input_1[0][0]']             \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 256),          369664    ['input_2[0][0]',             \n",
            "                              (None, 256),                           'lstm[0][1]',                \n",
            "                              (None, 256)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 104)            26728     ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 741480 (2.83 MB)\n",
            "Trainable params: 741480 (2.83 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9bB-mDF0qVI",
        "outputId": "a1eae22e-4cf6-41df-ee90-b81f31edc98f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "750/750 [==============================] - 21s 17ms/step - loss: 0.8408 - val_loss: 0.7620\n",
            "Epoch 2/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.5683 - val_loss: 0.6584\n",
            "Epoch 3/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.4994 - val_loss: 0.6047\n",
            "Epoch 4/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.4546 - val_loss: 0.5499\n",
            "Epoch 5/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.4219 - val_loss: 0.5176\n",
            "Epoch 6/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.3952 - val_loss: 0.4911\n",
            "Epoch 7/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.3748 - val_loss: 0.4738\n",
            "Epoch 8/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.3582 - val_loss: 0.4537\n",
            "Epoch 9/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.3444 - val_loss: 0.4399\n",
            "Epoch 10/50\n",
            "750/750 [==============================] - 11s 15ms/step - loss: 0.3326 - val_loss: 0.4304\n",
            "Epoch 11/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.3223 - val_loss: 0.4205\n",
            "Epoch 12/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.3133 - val_loss: 0.4089\n",
            "Epoch 13/50\n",
            "750/750 [==============================] - 11s 15ms/step - loss: 0.3053 - val_loss: 0.4044\n",
            "Epoch 14/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2979 - val_loss: 0.3966\n",
            "Epoch 15/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.2911 - val_loss: 0.3876\n",
            "Epoch 16/50\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.2850 - val_loss: 0.3853\n",
            "Epoch 17/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2793 - val_loss: 0.3801\n",
            "Epoch 18/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.2739 - val_loss: 0.3757\n",
            "Epoch 19/50\n",
            "750/750 [==============================] - 11s 15ms/step - loss: 0.2690 - val_loss: 0.3714\n",
            "Epoch 20/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2643 - val_loss: 0.3681\n",
            "Epoch 21/50\n",
            "750/750 [==============================] - 11s 15ms/step - loss: 0.2602 - val_loss: 0.3656\n",
            "Epoch 22/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2561 - val_loss: 0.3635\n",
            "Epoch 23/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2522 - val_loss: 0.3589\n",
            "Epoch 24/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2485 - val_loss: 0.3596\n",
            "Epoch 25/50\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.2451 - val_loss: 0.3564\n",
            "Epoch 26/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2417 - val_loss: 0.3555\n",
            "Epoch 27/50\n",
            "750/750 [==============================] - 11s 15ms/step - loss: 0.2387 - val_loss: 0.3524\n",
            "Epoch 28/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2356 - val_loss: 0.3509\n",
            "Epoch 29/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2327 - val_loss: 0.3494\n",
            "Epoch 30/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2300 - val_loss: 0.3481\n",
            "Epoch 31/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.2272 - val_loss: 0.3474\n",
            "Epoch 32/50\n",
            "750/750 [==============================] - 12s 16ms/step - loss: 0.2247 - val_loss: 0.3481\n",
            "Epoch 33/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2222 - val_loss: 0.3470\n",
            "Epoch 34/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2198 - val_loss: 0.3451\n",
            "Epoch 35/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2176 - val_loss: 0.3440\n",
            "Epoch 36/50\n",
            "750/750 [==============================] - 11s 15ms/step - loss: 0.2154 - val_loss: 0.3435\n",
            "Epoch 37/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.2131 - val_loss: 0.3425\n",
            "Epoch 38/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2108 - val_loss: 0.3435\n",
            "Epoch 39/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2088 - val_loss: 0.3432\n",
            "Epoch 40/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2068 - val_loss: 0.3413\n",
            "Epoch 41/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.2048 - val_loss: 0.3428\n",
            "Epoch 42/50\n",
            "750/750 [==============================] - 12s 15ms/step - loss: 0.2029 - val_loss: 0.3419\n",
            "Epoch 43/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.2009 - val_loss: 0.3420\n",
            "Epoch 44/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.1992 - val_loss: 0.3422\n",
            "Epoch 45/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.1973 - val_loss: 0.3420\n",
            "Epoch 46/50\n",
            "750/750 [==============================] - 11s 15ms/step - loss: 0.1956 - val_loss: 0.3413\n",
            "Epoch 47/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.1938 - val_loss: 0.3440\n",
            "Epoch 48/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.1922 - val_loss: 0.3430\n",
            "Epoch 49/50\n",
            "750/750 [==============================] - 10s 14ms/step - loss: 0.1906 - val_loss: 0.3438\n",
            "Epoch 50/50\n",
            "750/750 [==============================] - 11s 14ms/step - loss: 0.1890 - val_loss: 0.3432\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a3f5c4df0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기계 번역을 하도록 모델을 조정하고 동작시켜보도록 하겠습니다.\n",
        "# 전체적인 번역 동작 단계를 정리하면 아래와 같습니다.\n",
        "# 1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻습니다.\n",
        "# 2. 상태와 <SOS>에 해당하는 \\t를 디코더로 보냅니다.\n",
        "# 3. 디코더가 <EOS>에 해당하는 \\n이 나올 때까지 다음 문자를 예측하는 행동을 반복합니다.\n",
        "\n",
        "# 인코더 모델\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "# 디코더를 정의\n",
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
        "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
        "\n",
        "# 단어로부터 인덱스를 얻는 것이 아니라 인덱스로부터 단어를 얻을 수 있는 index_to_src와 index_to_tar를 만들었습니다.\n",
        "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
        "index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n",
        "print('index_to_src : ', index_to_src)\n",
        "print('index_to_tar : ', index_to_tar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PIHLZqR0tlG",
        "outputId": "8029b7bc-16c3-495a-d01e-1b0216038e14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index_to_src :  {1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: '?', 24: 'A', 25: 'B', 26: 'C', 27: 'D', 28: 'E', 29: 'F', 30: 'G', 31: 'H', 32: 'I', 33: 'J', 34: 'K', 35: 'L', 36: 'M', 37: 'N', 38: 'O', 39: 'P', 40: 'Q', 41: 'R', 42: 'S', 43: 'T', 44: 'U', 45: 'V', 46: 'W', 47: 'X', 48: 'Y', 49: 'Z', 50: 'a', 51: 'b', 52: 'c', 53: 'd', 54: 'e', 55: 'f', 56: 'g', 57: 'h', 58: 'i', 59: 'j', 60: 'k', 61: 'l', 62: 'm', 63: 'n', 64: 'o', 65: 'p', 66: 'q', 67: 'r', 68: 's', 69: 't', 70: 'u', 71: 'v', 72: 'w', 73: 'x', 74: 'y', 75: 'z', 76: 'é', 77: 'ï', 78: '’', 79: '€'}\n",
            "index_to_tar :  {1: '\\t', 2: '\\n', 3: ' ', 4: '!', 5: '\"', 6: '$', 7: '%', 8: '&', 9: \"'\", 10: '(', 11: ')', 12: ',', 13: '-', 14: '.', 15: '0', 16: '1', 17: '2', 18: '3', 19: '4', 20: '5', 21: '6', 22: '7', 23: '8', 24: '9', 25: ':', 26: '?', 27: 'A', 28: 'B', 29: 'C', 30: 'D', 31: 'E', 32: 'F', 33: 'G', 34: 'H', 35: 'I', 36: 'J', 37: 'K', 38: 'L', 39: 'M', 40: 'N', 41: 'O', 42: 'P', 43: 'Q', 44: 'R', 45: 'S', 46: 'T', 47: 'U', 48: 'V', 49: 'W', 50: 'X', 51: 'Y', 52: 'a', 53: 'b', 54: 'c', 55: 'd', 56: 'e', 57: 'f', 58: 'g', 59: 'h', 60: 'i', 61: 'j', 62: 'k', 63: 'l', 64: 'm', 65: 'n', 66: 'o', 67: 'p', 68: 'q', 69: 'r', 70: 's', 71: 't', 72: 'u', 73: 'v', 74: 'w', 75: 'x', 76: 'y', 77: 'z', 78: '\\xa0', 79: '«', 80: '»', 81: 'À', 82: 'Ç', 83: 'É', 84: 'Ê', 85: 'Ô', 86: 'à', 87: 'â', 88: 'ç', 89: 'è', 90: 'é', 91: 'ê', 92: 'ë', 93: 'î', 94: 'ï', 95: 'ô', 96: 'ù', 97: 'û', 98: 'œ', 99: '\\u2009', 100: '‘', 101: '’', 102: '\\u202f', 103: '‽'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # 입력으로부터 인코더의 상태를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>에 해당하는 원-핫 벡터 생성\n",
        "  target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "  target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = \"\"\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루프 반복\n",
        "  while not stop_condition:\n",
        "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 문자를 예측 문장에 추가\n",
        "    decoded_sentence += sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "    if (sampled_char == '\\n' or\n",
        "        len(decoded_sentence) > max_tar_len):\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence\n",
        "for seq_index in [5,500,1000]: # 입력 문장의 인덱스\n",
        "  input_seq = encoder_input[seq_index:seq_index+1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print(35 * \"-\")\n",
        "  print('입력 문장:', lines.src[seq_index])\n",
        "  print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
        "  print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16-eWbJJ6fOC",
        "outputId": "a2025dbe-86e9-4b4e-a144-08110b002ca7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "-----------------------------------\n",
            "입력 문장: Hi.\n",
            "정답 문장: Salut. \n",
            "번역 문장: Avez-vous. \n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-----------------------------------\n",
            "입력 문장: Can I go?\n",
            "정답 문장: Je peux y aller ? \n",
            "번역 문장: Puis-je faire ? \n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-----------------------------------\n",
            "입력 문장: Get going.\n",
            "정답 문장: En route ! \n",
            "번역 문장: Dégage ! \n"
          ]
        }
      ]
    }
  ]
}